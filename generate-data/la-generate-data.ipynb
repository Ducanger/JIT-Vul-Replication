{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"la\"\n",
    "COMMIT_METADATA_CSV = \"raw-data/commit_metadata.csv\"\n",
    "COMMIT_CODE_CSV = \"raw-data/commit_code.csv\"\n",
    "TRAIN_TEST_SPLIT_MODE = \"by_time\"  # by_time/cross_project\n",
    "\n",
    "TRAIN_TEST_SPLIT_DIR = f\"raw-data/split_data/{TRAIN_TEST_SPLIT_MODE}\"\n",
    "OUTPUT_DIR = f\"data/{PROJECT}/{TRAIN_TEST_SPLIT_MODE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded commit data from CSV files [Size: 20274]\n",
      "Building LAPredict data format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20274/20274 [01:26<00:00, 233.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training & testing data from size 20274\n",
      "Dumping PKL files to data/la/by_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_la_data_generator():\n",
    "    commit_metadata_df = pd.read_csv(COMMIT_METADATA_CSV)\n",
    "    commit_code_df = pd.read_csv(COMMIT_CODE_CSV)\n",
    "    print(f\"Loaded commit data from CSV files [Size: {len(commit_metadata_df)}]\")\n",
    "\n",
    "    la_train_df, la_test_df = get_la_data(commit_metadata_df, commit_code_df)\n",
    "\n",
    "    print(f\"Dumping PKL files to {OUTPUT_DIR}\")\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    la_train_df.to_pickle(os.path.join(OUTPUT_DIR, \"features_train.pkl\"))\n",
    "    la_test_df.to_pickle(os.path.join(OUTPUT_DIR, \"features_test.pkl\"))\n",
    "    \n",
    "def get_la_data(commit_metadata_df, commit_code_df):\n",
    "    print(f\"Building LAPredict data format\")\n",
    "    la_df = pd.DataFrame(columns=['commit_hash', 'is_buggy_commit', 'la', 'ld'])\n",
    "\n",
    "    for commit_id, vul_label in tqdm(commit_metadata_df[[\"commit_id\", \"vul_label\"]].values):\n",
    "        label = vul_label\n",
    "\n",
    "        commit_code_detail = commit_code_df[commit_code_df[\"commit_id\"] == commit_id]\n",
    "        added_code_num = 0\n",
    "        removed_code_num = 0\n",
    "        for diff_file in commit_code_detail.iterrows():\n",
    "            added_code = diff_file[1][1]\n",
    "            removed_code = diff_file[1][2]\n",
    "\n",
    "            if isinstance(removed_code, str):\n",
    "                removed_code_num += len(removed_code.splitlines())\n",
    "            if isinstance(added_code, str):\n",
    "                added_code_num += len(added_code.splitlines())\n",
    "        la_df.loc[len(la_df)] = [commit_id, label, added_code_num, removed_code_num]\n",
    "\n",
    "    training_data_df, testing_data_df = split_data(la_df)\n",
    "\n",
    "    return training_data_df, testing_data_df\n",
    "\n",
    "def split_data(input_data_df):\n",
    "    print(f\"Splitting training & testing data from size {len(input_data_df)}\")\n",
    "    train_ids = [l.strip() for l in open(os.path.join(TRAIN_TEST_SPLIT_DIR, \"train_ids.txt\")).readlines()]\n",
    "    test_ids = [l.strip() for l in open(os.path.join(TRAIN_TEST_SPLIT_DIR, \"test_ids.txt\")).readlines()]\n",
    "    assert len(set(train_ids + test_ids)) == len(input_data_df)\n",
    "    train_df, test_df = [x for _, x in input_data_df.groupby(~input_data_df['commit_hash'].isin(train_ids))]\n",
    "    train_df = train_df.set_index('commit_hash').loc[train_ids].reset_index()\n",
    "    test_df = test_df.set_index('commit_hash').loc[test_ids].reset_index()\n",
    "    return train_df, test_df\n",
    "run_la_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    commit_hash  is_buggy_commit   la   ld\n",
      "0      2b1702456dab2e8de6f74e8e4e03aac87571aafd                1  470   83\n",
      "1      04906bd5de2f220bf100b605dad37b4a1d9a91a6                0    1    0\n",
      "2      2864e767053317538feafa815046fff89e5a16be                0   77   43\n",
      "3      795b859eee96c700e8f3c3fe68e6a9a39d95797c                1  940  431\n",
      "4      c0f8b0470cbc3707993db02a81f4356e294adcf1                1   30   45\n",
      "...                                         ...              ...  ...  ...\n",
      "16214  98da63b3f5f5a277c5c3a16860db9a9f6741e54c                0    1    1\n",
      "16215  5d996b56499f00f80b02a41bab3d6b7349e36e9d                0   10    0\n",
      "16216  a84d610b372c63e8a48a9ed7c038a2954097512c                0    4    1\n",
      "16217  23f3f92361a3db53e595de33cfd5440f53bee220                0    5    5\n",
      "16218  7bc4f0846f5e15dad5a54490290241243b5a4416                0    1    0\n",
      "\n",
      "[16219 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open(f'data/{PROJECT}/{TRAIN_TEST_SPLIT_MODE}/features_train.pkl', 'rb'))\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
