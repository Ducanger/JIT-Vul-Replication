{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"jitfine\"\n",
    "COMMIT_METADATA_CSV = \"raw-data/commit_metadata.csv\"\n",
    "METRICS_CSV = \"raw-data/commit_expert_features_14_metrics.csv\"\n",
    "COMMIT_CODE_CSV = \"raw-data/commit_code.csv\"\n",
    "TRAIN_TEST_SPLIT_MODE = \"by_time\"  # by_time/cross_project\n",
    "\n",
    "TRAIN_TEST_SPLIT_DIR = f\"raw-data/split_data/{TRAIN_TEST_SPLIT_MODE}\"\n",
    "OUTPUT_DIR = f\"data/{PROJECT}/{TRAIN_TEST_SPLIT_MODE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded commit data from CSV files [Size: 20274]\n",
      "Building JITFine data format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20274/20274 [01:13<00:00, 275.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training & testing data from size 20274\n",
      "Dumping PKL files to data/jitfine/by_time\n"
     ]
    }
   ],
   "source": [
    "def run_jit_data_generator():\n",
    "    commit_metadata_df = pd.read_csv(COMMIT_METADATA_CSV)\n",
    "\n",
    "    commit_expert_feature_df = pd.read_csv(METRICS_CSV)\n",
    "    commit_expert_feature_df.columns = [col_name.lower() for col_name in\n",
    "                                        commit_expert_feature_df.columns.values.tolist()]\n",
    "    commit_expert_feature_df.rename(columns={'commit_id': 'commit_hash'}, inplace=True)\n",
    "    commit_expert_feature_df = commit_expert_feature_df[\n",
    "        ['index', 'commit_hash', 'la', 'ld', 'nf', 'ns', 'nd', 'entropy',\n",
    "         'ndev', 'lt', 'nuc', 'age', 'exp', 'rexp', 'sexp', 'fix']]\n",
    "    commit_code_df = pd.read_csv(COMMIT_CODE_CSV)\n",
    "\n",
    "    print(f\"Loaded commit data from CSV files [Size: {len(commit_metadata_df)}]\")\n",
    "\n",
    "    code_change_data_objs, expert_feature_data_dfs = get_cc2vec_data(commit_metadata_df, commit_expert_feature_df,\n",
    "                                                                     commit_code_df)\n",
    "\n",
    "    ef_train_df, ef_val_df, ef_test_df = expert_feature_data_dfs\n",
    "    cc_train_obj, cc_val_obj, cc_test_obj = code_change_data_objs\n",
    "\n",
    "    print(f\"Dumping PKL files to {OUTPUT_DIR}\")\n",
    "\n",
    "    # dump Expert Features with Pandas Dataframe pickles\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    ef_train_df.to_pickle(os.path.join(OUTPUT_DIR, \"features_train.pkl\"))\n",
    "    ef_val_df.to_pickle(os.path.join(OUTPUT_DIR, \"features_valid.pkl\"))\n",
    "    ef_test_df.to_pickle(os.path.join(OUTPUT_DIR, \"features_test.pkl\"))\n",
    "\n",
    "    # dump Code Changes with Python pickles\n",
    "    with open(os.path.join(OUTPUT_DIR, \"changes_train.pkl\"), 'wb') as f:\n",
    "        pickle.dump(cc_train_obj, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"changes_valid.pkl\"), 'wb') as f:\n",
    "        pickle.dump(cc_val_obj, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"changes_test.pkl\"), 'wb') as f:\n",
    "        pickle.dump(cc_test_obj, f)\n",
    "\n",
    "\n",
    "def get_cc2vec_data(commit_metadata_df, commit_expert_feature_df, commit_code_df):\n",
    "    print(f\"Building JITFine data format\")\n",
    "    ids, labels, msgs, codes = [], [], [], []\n",
    "\n",
    "    for commit_id, vul_label, commit_msg in tqdm(\n",
    "            commit_metadata_df[[\"commit_id\", \"vul_label\", \"commit_message\"]].values):\n",
    "        label = vul_label\n",
    "\n",
    "        if not isinstance(commit_msg, str):\n",
    "            commit_msg = \"\"\n",
    "        msg = split_sentence(commit_msg)\n",
    "        msg = ' '.join(msg.split(' ')).lower()\n",
    "\n",
    "        commit_code_detail = commit_code_df[commit_code_df[\"commit_id\"] == commit_id]\n",
    "        files_changed_code = {\"added_code\": set(), \"removed_code\": set()}\n",
    "        for diff_file in commit_code_detail.iterrows():\n",
    "            added_code = diff_file[1][1]\n",
    "            removed_code = diff_file[1][2]\n",
    "\n",
    "            if isinstance(removed_code, str):\n",
    "                for line in removed_code.splitlines():\n",
    "                    removed_line = line.strip()\n",
    "                    removed_line = ' '.join(split_sentence(removed_line).split())\n",
    "                    files_changed_code[\"removed_code\"].add(removed_line)\n",
    "            if isinstance(added_code, str):\n",
    "                for line in added_code.splitlines():\n",
    "                    added_line = line.strip()\n",
    "                    added_line = ' '.join(split_sentence(added_line).split())\n",
    "                    files_changed_code[\"added_code\"].add(added_line)\n",
    "\n",
    "        ids.append(commit_id)\n",
    "        labels.append(label)\n",
    "        msgs.append(msg)\n",
    "        codes.append(files_changed_code)\n",
    "\n",
    "    code_data = [ids, labels, msgs, codes]\n",
    "\n",
    "    code_data_splits, expert_feature_df_splits = split_data(code_data, commit_expert_feature_df)\n",
    "\n",
    "    return code_data_splits, expert_feature_df_splits\n",
    "\n",
    "\n",
    "def split_data(code_data, commit_expert_feature_df):\n",
    "    print(f\"Splitting training & testing data from size {len(code_data[0])}\")\n",
    "    data_id_to_index_dict = {c_id: i for i, c_id in enumerate(code_data[0])}\n",
    "    train_ids = [l.strip() for l in open(os.path.join(TRAIN_TEST_SPLIT_DIR, \"train_ids.txt\")).readlines()]\n",
    "    test_ids = [l.strip() for l in open(os.path.join(TRAIN_TEST_SPLIT_DIR, \"test_ids.txt\")).readlines()]\n",
    "    # assert len(set(train_ids + test_ids)) == len(code_data[0])\n",
    "    train_ids = list(set(train_ids) & set(code_data[0]))\n",
    "    test_ids = list(set(test_ids) & set(code_data[0]))\n",
    "    training_data, val_data, testing_data = [[], [], [], []], [[], [], [], []], [[], [], [], []]\n",
    "    ef_train_df, ef_test_df = [x for _, x in commit_expert_feature_df.groupby(\n",
    "        ~commit_expert_feature_df['commit_hash'].isin(train_ids))]\n",
    "\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=0.20, shuffle=False)\n",
    "    ef_train_df, ef_val_df = [x for _, x in ef_train_df.groupby(~ef_train_df['commit_hash'].isin(train_ids))]\n",
    "\n",
    "    ef_train_df = ef_train_df.set_index('commit_hash').loc[train_ids].reset_index()\n",
    "    ef_val_df = ef_val_df.set_index('commit_hash').loc[val_ids].reset_index()\n",
    "    ef_test_df = ef_test_df.set_index('commit_hash').loc[test_ids].reset_index()\n",
    "\n",
    "    for _id in train_ids:\n",
    "        index = data_id_to_index_dict[_id]\n",
    "        training_data[0].append(code_data[0][index])\n",
    "        training_data[1].append(code_data[1][index])\n",
    "        training_data[2].append(code_data[2][index])\n",
    "        training_data[3].append(code_data[3][index])\n",
    "\n",
    "    for _id in val_ids:\n",
    "        index = data_id_to_index_dict[_id]\n",
    "        val_data[0].append(code_data[0][index])\n",
    "        val_data[1].append(code_data[1][index])\n",
    "        val_data[2].append(code_data[2][index])\n",
    "        val_data[3].append(code_data[3][index])\n",
    "\n",
    "    for _id in test_ids:\n",
    "        index = data_id_to_index_dict[_id]\n",
    "        testing_data[0].append(code_data[0][index])\n",
    "        testing_data[1].append(code_data[1][index])\n",
    "        testing_data[2].append(code_data[2][index])\n",
    "        testing_data[3].append(code_data[3][index])\n",
    "\n",
    "    return (training_data, val_data, testing_data), (ef_train_df, ef_val_df, ef_test_df)\n",
    "\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    sentence = sentence.replace('.', ' . ').replace('_', ' ').replace('@', ' @ ') \\\n",
    "        .replace('-', ' - ').replace('~', ' ~ ').replace('%', ' % ').replace('^', ' ^ ') \\\n",
    "        .replace('&', ' & ').replace('*', ' * ').replace('(', ' ( ').replace(')', ' ) ') \\\n",
    "        .replace('+', ' + ').replace('=', ' = ').replace('{', ' { ').replace('}', ' } ') \\\n",
    "        .replace('|', ' | ').replace('\\\\', ' \\ ').replace('[', ' [ ').replace(']', ' ] ') \\\n",
    "        .replace(':', ' : ').replace(';', ' ; ').replace(',', ' , ').replace('<', ' < ') \\\n",
    "        .replace('>', ' > ').replace('?', ' ? ').replace('/', ' / ')\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return sentence\n",
    "\n",
    "run_jit_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    commit_hash  index   la  ld  nf  ns  nd  \\\n",
      "0      d208d1eba3799c58fd6d3602d31de3e686f14aec   7584    4  10   1   1   1   \n",
      "1      5d7743019b327b3333947f5e96ca6289654c4aa7   2606  415   1   2   2   2   \n",
      "2      54a6c11b20bb635ac5bb5d9369782bf00d0c7e19   5396    8   0   1   1   1   \n",
      "3      444bc908611ccaf4512dc37c33ac3b54d873a62b   4790   16   8   1   1   1   \n",
      "4      1a6245a5b0b4e8d822c739b403fc67c8a7bc8d12   3233  103  47   1   1   1   \n",
      "...                                         ...    ...  ...  ..  ..  ..  ..   \n",
      "12970  12abac8bb78c494597d740e7f9afd202f63180a3     42   58  34   1   1   1   \n",
      "12971  079d0b7f1eedcc634c371fe05b617fdc55c8b762   1393   91  86   2   2   2   \n",
      "12972  57ec555e8ef3c5ef1d77d48dc7cc868e56ddadc9   3658   17   2   1   1   1   \n",
      "12973  cb077b7aa319caf4a11e811df93b1c2b86fff954   4744    0   2   1   1   1   \n",
      "12974  2d83f323d63332e5ecaa481d8f9301c0ea92b6ba   1915    9  10   1   1   1   \n",
      "\n",
      "        entropy  ndev     lt  nuc     age   exp         rexp  sexp  fix  \n",
      "0      0.073218     7    851    1     0.0  3060  4905.083333    14    0  \n",
      "1      0.431343    54    434    1   693.0  2290  2299.233333    11    0  \n",
      "2      0.012553    16   2484    1  2368.0    45   114.000000    10    0  \n",
      "3      0.044446    32   2494    1     6.0   767  1537.916667    39    0  \n",
      "4      0.314272     3   1052    1     1.0   491   644.166667    19    0  \n",
      "...         ...   ...    ...  ...     ...   ...          ...   ...  ...  \n",
      "12970  0.331138    89   3210    2   170.5  1413  4715.500000     8    0  \n",
      "12971  0.721329    61  21855    9   389.0   705   887.333333    12    0  \n",
      "12972  0.117695    11    515    1   739.0   103   213.000000     8    1  \n",
      "12973  0.001978    47   8028    1     0.0  3888  2663.973810    13    1  \n",
      "12974  0.040111     2   2391    1     0.0     4     4.000000     2    0  \n",
      "\n",
      "[12975 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open(f'data/{PROJECT}/{TRAIN_TEST_SPLIT_MODE}/features_train.pkl', 'rb'))\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
